{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c40548-89f9-4114-8ffa-f7cfeed7b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ClusterTree.ipynb\n",
    "# %run losses.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b492a6-ddd9-4358-968c-c4d4b3cc8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # Convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Convolutional layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 128),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6)  # Latent space layer\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(6, 128),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32*7*7),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32, 7, 7)),  # Reshape to feature maps\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Deconvolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Deconvolutional layer 2\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode input into latent space\n",
    "        latent = self.encoder(x)\n",
    "\n",
    "        # Decode latent representation\n",
    "        reconstruction = self.decoder(latent)\n",
    "\n",
    "        return reconstruction , latent \n",
    "\n",
    "# Instantiate the model\n",
    "model = EncoderDecoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424e57c8-2ce2-4b4b-be3e-d5b2f5fdacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainset.data = trainset.data[:2000]\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144592a3-2e4e-4179-895d-1873abc27c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, num_images=640):\n",
    "        self.dataset = dataset\n",
    "        self.num_images = num_images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img, _ = self.dataset[index]  # Get image and ignore label\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2f3667-f75f-443f-a7f7-aed0356ed544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_no_labels = TestDataset(trainset)\n",
    "train_dataset_no_labels.__len__()\n",
    "# num_images = 2000\n",
    "# mnist_subset = torch.utils.data.Subset(train_dataset_no_labels, range(num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6dbd05-a582-4d34-a989-02733d33aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset_no_labels, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451170fb-4b46-470c-8390-f1287bfde060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     8] loss: 0.017\n",
      "[1,    16] loss: 0.014\n",
      "[1,    24] loss: 0.010\n",
      "[1,    32] loss: 0.008\n",
      "[2,     8] loss: 0.007\n",
      "[2,    16] loss: 0.007\n",
      "[2,    24] loss: 0.007\n",
      "[2,    32] loss: 0.007\n",
      "[3,     8] loss: 0.006\n",
      "[3,    16] loss: 0.006\n",
      "[3,    24] loss: 0.005\n",
      "[3,    32] loss: 0.005\n",
      "[4,     8] loss: 0.005\n",
      "[4,    16] loss: 0.005\n",
      "[4,    24] loss: 0.004\n",
      "[4,    32] loss: 0.004\n",
      "[5,     8] loss: 0.004\n",
      "[5,    16] loss: 0.004\n",
      "[5,    24] loss: 0.004\n",
      "[5,    32] loss: 0.003\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Pretraining \n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs , latent = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 8 == 7:  # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65010a04-52f9-46f8-aa05-4219ed850ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model_weight_pretrained_2000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecbce2f-89b0-43c5-9657-049039aee72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_weight_pretrained_2000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db442052-459c-482f-8ae2-ec55628572c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_model_output(model, dataloader, index):\n",
    "    # Get the dataset from the dataloader\n",
    "    dataset = dataloader.dataset\n",
    "\n",
    "    # Get the input image from the dataset\n",
    "    img = dataset.dataset[index][0]  # Access the dataset within the DataLoader\n",
    "    print(img.shape)\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output , _ = model(img.unsqueeze(0))\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "    # Convert torch tensors to numpy arrays\n",
    "    img = img.numpy()\n",
    "    output = output.numpy()\n",
    "\n",
    "    # Plot the original image and the model's output\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img[0], cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output[0], cmap='gray')\n",
    "    plt.title('Model Output')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b9b05d-7f8c-46ba-b15d-57ad4e8bf7ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_model_output(model, train_loader, 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c5dbbc-2fbe-4e1c-a547-95400d2578f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to incorporate the rest of the losses and tree formation\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "trainset2 = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainset2.data = trainset2.data[:2000]\n",
    "un_train_dataset_no_labels = TestDataset(trainset2)\n",
    "un_train_loader = torch.utils.data.DataLoader(un_train_dataset_no_labels, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9cd8b53-ceea-43bf-b173-6d16820fc222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x285a52fb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f30c2-5a42-4fb1-bebc-005c97bfa25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ea78b-7c82-494d-8886-d752a01e2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[1,     2] loss: 0.015715\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "[1,     4] loss: 0.665108\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "[1,     6] loss: 0.488776\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "[1,     8] loss: 0.293009\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "[1,    10] loss: 0.284798\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "[1,    12] loss: 0.311065\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "[1,    14] loss: 0.363525\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "[1,    16] loss: 0.354195\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "[1,    18] loss: 0.339748\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "[1,    20] loss: 0.308848\n",
      "Iteration 19\n",
      "Iteration 20\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "   \n",
    "    for i, data in enumerate(un_train_loader, 0):\n",
    "        \n",
    "        inputs = data\n",
    "       \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs , latent = model(inputs)\n",
    "        \n",
    "        latent = latent.detach().numpy()\n",
    "        if i==0:\n",
    "            clust_tree = ClusterTree(D = latent , L = 10 , T = 2 ,P = 0 )\n",
    "\n",
    "        else:\n",
    "            clust_tree.build_tree(latent)\n",
    "            \n",
    "            \n",
    "            \n",
    "        if i==0:\n",
    "            continue\n",
    "        # Compute loss\n",
    "        mse_loss = criterion(outputs , inputs)\n",
    "\n",
    "        loss_nc = clust_tree.calculate_loss_NC()\n",
    "\n",
    "        loss_dc = clust_tree.calculate_loss_DC()\n",
    "\n",
    "        loss = mse_loss + loss_nc + loss_dc\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:  # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch + 1, i + 1, running_loss / 2))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175ea7c-6f8c-4def-a18e-286df59ff514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0935a8e4-faa1-4ba7-b377-a01542794d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/learning/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/learning/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/learning/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/learning/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m, in \u001b[0;36mTestDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_images:\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex out of range\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     img, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index]  \u001b[38;5;66;03m# Get image and ignore label\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mIndexError\u001b[0m: Index out of range"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "    input = data\n",
    "\n",
    "    print(input.shape)\n",
    "    \n",
    "    out , point = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69266256-8aaa-46ce-bc56-30302f096b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
